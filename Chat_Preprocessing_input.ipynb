import csv
import re
import nltk
from transformers import AutoTokenizer
import spacy

# Ensure you have downloaded the required NLTK resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Load SpaCy model for NER
nlp = spacy.load("en_core_web_sm")

# Initialize BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)

# Function to clean text
def clean_text(text):
    return re.sub(r'[^\w\s]', '', text.lower())

# Function to remove unnecessary tokens, stopwords and names
def clean_tokens(tokens):
    # Process tokens with SpaCy to identify named entities
    doc = nlp(" ".join(tokens))

    # Extract names from NER results from SpaCy
    named_entities_spacy = {ent.text.lower() for ent in doc.ents if ent.label_ == 'PERSON'}

    # Filter out named entities from tokens
    final_cleaned_tokens = [token for token in tokens if token.lower() not in named_entities_spacy and token.isalpha()]

    return final_cleaned_tokens

# Improved function to extract rate
def extract_rate(text):
    rate_pattern = r'\b\d{1,4}(?:\.\d{1,4})?\b'
    rates = re.findall(rate_pattern, text)
    rates = [float(rate) for rate in rates if 100 < float(rate) < 1000]
    return rates[0] if rates else None

# Function to determine sentiment
def get_sentiment(text):
    positive_words = ['good', 'ok', 'agreed', 'appreciate', 'deal', 'done', 'smooth']
    negative_words = ['tight', 'below', "isn't", 'comfortable', 'not', "can't"]

    positive_count = sum(1 for word in positive_words if word in text.lower())
    negative_count = sum(1 for word in negative_words if word in text.lower())

    if positive_count > negative_count:
        return 'Positive'
    elif negative_count > positive_count:
        return 'Negative'
    else:
        return 'Neutral'

# Function to read chat from file
def read_chat_from_file(file_path):
    chat_lines = []
    current_speaker = ""
    current_timestamp = ""
    current_message = ""

    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            line = line.strip()
            if line.startswith("**") and line.endswith(":**"):
                if current_speaker and current_timestamp and current_message:
                    chat_lines.append((current_timestamp, current_speaker, current_message.strip()))
                current_speaker = line.strip("**:")
                current_message = ""
            elif line.startswith("*") and line.endswith("*"):
                current_timestamp = line.strip("*")
            else:
                current_message += " " + line

        if current_speaker and current_timestamp and current_message:
            chat_lines.append((current_timestamp, current_speaker, current_message.strip()))

    return chat_lines

# Process the chat and write to CSV
csv_content = [['Timestamp', 'Speaker', 'Cleaned_Message', 'Token_Category', 'Rate', 'Sentiment', 'BERT_Tokens']]

# Replace with the actual path to your text file
file_path = 'T1.txt'
chat_lines = read_chat_from_file(file_path)

for timestamp, speaker, message in chat_lines:
    cleaned_message = clean_text(message)
    rate = extract_rate(message)
    sentiment = get_sentiment(message)

    # Use BERT tokenizer
    bert_tokens = tokenizer.tokenize(message)  # Cleaned: Removed clean_up_tokenization_spaces
    cleaned_tokens = clean_tokens(bert_tokens)  # Clean the tokens by removing names

    if 'USD' in message or 'NGN' in message:
        token_category = 'Currency'
    elif '$' in message or 'million' in message:
        token_category = 'Amount'
    elif 'rate' in message.lower():
        token_category = 'Rate Inquiry'
    elif rate:
        token_category = 'Rate Offer'
    else:
        token_category = 'General'

    csv_content.append([
        timestamp,
        speaker,
        cleaned_message,
        token_category,
        rate if rate else '',
        sentiment,
        ', '.join(cleaned_tokens)  # Store cleaned tokens as a string
    ])

# Write to CSV file
output_file = 'processed_chat.csv'

with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(csv_content)

print(f"CSV file '{output_file}' has been created successfully.")